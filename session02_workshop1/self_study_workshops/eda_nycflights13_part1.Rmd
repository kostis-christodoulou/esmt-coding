---
title: 'Setting the stage for data science: integration of data management skill using the New York flights package (nycflights13)'
author: "Nicholas J. Horton, Benjamin S. Baumer, and Hadley Wickham"
date: "March 31, 2015"
output:
  html_document:
    fig_height: 3
    fig_width: 5
    toc: yes
    toc_float: yes
  word_document:
    fig_height: 3
    fig_width: 5
    toc: yes
---


```{r include=FALSE}
require(tidyverse)
require(mosaic)
require(ggformula)
trellis.par.set(theme=theme.mosaic())  

# knitr settings to control how R chunks work.
require(knitr)
opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small"    # slightly smaller font for code
)
```

```{r, include=FALSE}
# Load additional packages here.  Uncomment the line below to use Project MOSAIC data sets.
# require(mosaicData)   
```

Many have argued that we need additional facility to express statistical computations. 
By using commonplace tools for data management, visualization, and reproducible analysis in data science and applying these to real-world scenarios, we prepae to think statistically.  In an era of increasingly big data, it is imperative that we develop data-related capacities, beginning early on. We believe that the integration of these precursors to data science into our curricula-early and often-will help statisticians be part of the dialogue regarding *Big Data and Big Questions*.

Specifically, through our shared experience working in industry, government, private consulting, and academia we have identified five key elements which deserve greater emphasis (in no particular order):

  #. Thinking creatively, but constructively, about data. This "data tidying" includes the ability to move data not only between different file formats, but also different *shapes*. There are elements of data storage design (e.g. normal forms) and foresight into how data should arranged based on how it will likely be used. 
  #. Facility with data sets of varying sizes and some understanding of scalability issues when working with data. This includes an elementary understanding of basic computer architecture (e.g. memory vs. hard disk space), and the ability to query a relational database management system (RDBMS). 
  #. Statistical computing skills in a command-driven environment (e.g. R, Python, or Julia). Coding skills (in any language) are highly-valued and increasingly necessary. They provide freedom from the un-reproducible point-and-click application paradigm that programs like SPPS and Excel offer. 
  #. Experience wrestling with large, messy, complex, challenging data sets, for which there is no obvious goal or specially-curated statistical method (see SIDEBAR: What's in a name). While perhaps suboptimal for teaching specific statistical methods, these data are more similar to what analysts actually see in the real world; they use wild rather than tidy data. 
  #. An ethos of reproducibility. This is a major challenge for science in general, and we have the comparatively easy task of simply reproducing computations and analysis through the use of R Markdown Documents (RMD)

We illustrate how these five elements can be addressed using the `nycflights13` package, which includes five dataframes (airlines, airports, planes, flights, weather) that can be accessed within R).  

```{r, echo=TRUE, eval=TRUE, message=FALSE}
require(nycflights13)
airlines
airports
planes
flights
weather
```


#### A framework for data-related skills

The statistical data analysis cycle involves the formulation of questions, collection of data, analysis, and interpretation of results (see Figure 1).  Data preparation and manipulation is not just a first step, but a key component of this cycle (which will often be nonlinear, see also http://www.jstatsoft.org/v59/i10/paper).  When working with data, analysts must first determine what is needed, describe this solution in terms that a computer can understand, and execute the code.  


Here we illustrate how the `dplyr` package in R (http://cran.r-project.org/web/packages/dplyr) can be used to build a powerful and broadly accessible foundation for data manipulation. This approach is attractive because it provides simple functions that correspond to the most common data manipulation operations (or *verbs*) and uses efficient storage approaches so that the analyst can focus on the analysis. 

```
verb          meaning
--------------------------------------------
select()      select variables (or columns)
filter()      subset observations (or rows)
mutate()      add new variables (or columns)
arrange()     re-order the observations
summarise()   reduce to a single row
group_by()    aggregate
left_join()   merge two data objects
distinct()    remove duplicate entries
collect()     force computation and bring data back into R
```
Table 1: Key verbs in `dplyr` and `tidyr` to support data management and manipulation 


#### Airline delays

We demonstrate how to undertake analysis using the tools in the `dplyr` package. A smaller dataset is available for n=336,776 New York City flights in 2013 within the `nycflights13` package.  The interface in R accessing the full database is almost identical in terms of the `dplyr` functionality, with the same functions being used.

You use this dataset to address questions that they find real and relevant. (It is not hard to find motivation for investigating patterns of flight delays. Just think: have you ever been stuck in an airport because your flight was delayed or cancelled and wondered if you could have predicted the delay if you'd had more data?)


#### Filtering observations

We start with an analysis focused on three smaller airports in the Northeast.  This illustrates the use of `filter()`, which allows the specification of a subset of rows of interest in the `airports` table (or dataset).  We first start by exploring the `airports` table.  Suppose we wanted to find out which airports certain codes belong to?

```{r warning=FALSE}
filter(airports, faa %in% c('ALB', 'BDL', 'BTV'))   
```

#### Aggregating observations

Next we aggregate the counts of flights at all three of these airports at the monthly level (in the `ontime` flight-level table), using the `group_by()` and `summarise()` functions.   The `collect()` function forces the evaluation.  These functions are connected using the `%>%` operator.  This pipes the results from one object or function as input to the next in an efficient manner.
```{r}
airportcounts <- flights %>% 
   filter(dest %in% c('ALB', 'BDL', 'BTV')) %>%
   group_by(year, month, dest) %>%
   summarise(count = n()) %>% 
   collect()
```

#### Creating new derived variables

Next we add a new column by constructing a date variable (using `mutate()` and helper functions from the `lubridate` package), then generate a time series plot. 

```{r, message=FALSE}
library(lubridate)
airportcounts <- airportcounts %>%
  mutate(Date = ymd(paste(year, "-", month, "-01")))
head(airportcounts) # list only the first six observations


ggplot(airportcounts, aes(x=Date, y=count, colour=dest))+
  geom_line()+
  labs(
    x = "Year",
    y = "Number of flights per month"
  ) +
  theme_bw()+
  NULL
```

Figure 2: Comparison of the number of flights arriving at three airports by month in 2013.

We observe in Figure 2 that there are some interesting patterns over time for these airports.  Burlington has the largest number of flights. 

#### Sorting and selecting

Another important verb is `arrange()`, which in conjunction with `head()` lets us display the months with the largest number of flights.  Here we need to use `ungroup()`, since otherwise the data would remain aggregated by year, month, and destination.
```{r}
airportcounts %>%
  ungroup() %>%
  arrange(desc(count)) %>% 
  select(count, year, month, dest) %>% 
  head() 
```

We can compare flight delays between two airlines serving a city pair.  For example, which airline was most reliable flying from New York to Minneapolis/St. Paul (MSP) in January, 2013?  Here we demonstrate how to calculate an average delay for each day . We create the analytic dataset through use of ${\tt select()}$ (to pick the variables to be included), `filter()` (to select a tiny subset of the observations), and then repeat the previous aggregation. 

```{r, warning=FALSE}
delays <- flights %>% 
  select(origin, dest, year, month, day, carrier, arr_delay) %>%
  filter(dest == 'MSP' & month == 1) %>%
  group_by(year, month, day, carrier) %>%
  summarise(meandelay = mean(arr_delay), count = n()) 
options(digits=3)
favstats(~ meandelay, data=delays)

ggplot(delays, aes(x=carrier, y = meandelay))+
  geom_boxplot()+
  theme_bw()+
  NULL

```

#### Merging 

Merging is another key capacity to master.  Here, the full carrier names are merged (or joined, in database parlance) to facilitate the comparison, using the `left_join()` function to provide the actual airline name, rather than the terse airline code in the legend of the figure.

```{r, warning=FALSE}
merged <- left_join(delays, airlines, by=c("carrier" = "carrier")) 
head(merged) 
favstats(meandelay ~ name, data=merged)


ggplot(merged, aes(x=name, y = meandelay))+
  geom_boxplot()+
  theme_bw()+
  NULL

ggplot(merged, aes(x= meandelay, fill=name, alpha = 0.3))+
  geom_density()+
  labs(
   x="Average daily delay by carrier (in minutes)" 
  )+
  theme_bw()+
  NULL

```

Figure 3: Comparison of mean flight delays from New York to Minneapolis/St. Paul in January, 2013 

We see in Figure 3 that the airlines are fairly reliable, though there were some days with average delays of 60 minutes or more.
```{r}
merged %>% 
  filter(meandelay > 60) %>% 
  arrange(desc(meandelay))
```

#### Further wrangling and manipulation 

Other data wrangling and manipulation capacities can be developed using this dataset, including more elaborate data joins/merges (since there are tables providing additional (meta)data about planes).  As an example, consider the many flights of plane N355NB, which flew out of Bradley airport in January, 2008.  

```{r}
filter(planes, tailnum=="N355NB")
```

We see that this is an Airbus 319-- how often did this specific airplane fly through NYC in 2013?

```{r}
singleplane <- filter(flights, tailnum=="N355NB") %>% 
  select(year, month, day, dest, origin, distance) 
head(singleplane)
sum(~ distance, data=singleplane)
```
This Airbus A319 has been very active, with 128 flights just in 2013 in the New York City area.

```{r}
singleplane %>%
  group_by(dest) %>%
  summarise(count = n()) %>% 
  arrange(desc(count)) %>%
  filter(count > 5)
```

### Weather

Linkage to other data scraped from the Internet (e.g. detailed weather information for a particular airport or details about individual planes) may allow other questions to be answered.

```{r}
head(weather)

avgdelay <- flights %>%
  group_by(month, day) %>%
  filter(month < 13) %>%
  summarise(avgdelay = mean(arr_delay, na.rm=TRUE)) 

precip <- weather %>%
  group_by(month, day) %>%
  filter(month < 13) %>%
  summarise(totprecip = sum(precip), maxwind = max(wind_speed))

precip <- mutate(precip, anyprecip = ifelse(totprecip==0, "No", "Yes"))

merged <- left_join(avgdelay, precip, by=c("day", "month"))
head(merged)
```

A dramatic outlier emerges: windspeeds of 1000 mph are not common!
```{r}
favstats(~maxwind, data=merged)
filter(merged, maxwind > 1000)
merged <- filter(merged, maxwind < 1000)

ggplot(merged, aes(x=anyprecip, y= avgdelay))+
  geom_boxplot()+
  theme_bw()+
  NULL

ggplot(merged, aes(x=maxwind, y= avgdelay))+
  geom_point()+
  geom_smooth()+
  theme_bw()+
  NULL

ggplot(merged, aes(x=maxwind, y= avgdelay))+
  geom_point()+
  geom_smooth()+
  facet_wrap(~anyprecip)+
  theme_bw()+
  NULL


ggplot(merged, aes(x=avgdelay, fill=anyprecip, alpha = 0.3))+
  geom_density()+
  facet_wrap(~anyprecip)+
  labs(
    x="Average daily delay by carrier (in minutes)"  
  )+
  theme_bw()+
  theme(legend.position="none")+
  NULL
```

Is there a relationship between average delay times and wind speed? what is the best visualisation that the relationship is stronger when there is any precipitation? 





